# Deep Learning and Medical Image Segmentation

## Background

​	In just the last century, it can be argued that our understanding of medicine and treatment of disease has advanced more than it has likely done so in much of previous human history combined. Much of the most advanced medical imaging modalities are very expensive to acquire and maintain, meaning hospitals may only have one piece of equipment for some modalities like MRI. This can lead to long delays in patient treatment or even to an inaccessibility to this equipment in many places. Other modalities, such as ultrasound and x-ray, are much more ubiquitous, yet they suffer drawbacks in resolution and a loss of dimensional information due to the nature of how they image. In many cases, however, the modalities are still reliable enough for diagnosing common pathology.. 

​	Many improvements can still be made, especially with respect to how this medical image data is processed. With all medical imaging modalities, the rate determining step is image analysis by the doctors which leads to eventual diagnosis. Though diagnostic equipment may be cutting edge, medical image analysis is still quite archaic in sense that doctors must process images manually instead. One of the key areas of improvement for medical image analysis is image segmentation which is the process of generating pixel agglomerations from an image that  constitute parts of depicted objects and often involves to the  delineations of specific structure within the body. 

​	What's needed is to automate a large part of the image segmentation process in order to create an aid for medical professionals. These automated solutions are expected to greatly reduce human error, improve the accuracy and precision of timely diagnosis and even pave  the way for automated diagnostic software which can analysis images and provide meaningful insight. The challenge to this is programming the necessary logic to delineate structures in the human body. The inhomogeneity of tissues within organs also presents a major challenge, especially for more traditional segmentation methods such as edge-detection filters and mathematical techniques. Compounding these difficulties is the tendency for medical images to be prone to noise and artifacts.

​	Machine learning image segmentation methods notably improve segmentation accuracy in addition to making the segmentation and analysis of large image datasets much more practical. In a nutshell, these approaches use computer algorithms which automatically improve with experience. Most segmentation methods typically fall into one of two categories: supervised segmentation methods and unsupervised segmentation methods, though others do exist. Supervised segmentation methods typically use artificial neural networks to infer a function that maps input data to output data; they learn how to map input data to its most probable output. Supervised methods are advantageous in that they have great capacity for adaptive learning and can be tested in real time. However, they require extensive training time which can make them inefficient to use. Unsupervised segmentation methods do not require training and are able to classify images based on input alone. They use some metric chosen by the data scientist to group data based on similarity. Naturally, they are very useful with clustering algorithms. 

​	In both supervised and unsupervised approaches, it can be difficult to decide which model is best suited for a specific task. Achieving good results can be tedious and inefficient. Both methods also require the data scientist to implement some form of feature extraction and/or feature engineering, which may require them to have some prior knowledge and understanding of medical imaging. These methods are also sensitive to noise and have limited efficacy when segmenting ultrasound and CT scan images as a result. Additionally, no single technique solves all segmentation problems and their usefulness is dependent on the imaging modality being used.

​	Deep learning is a relatively recent family of machine learning paradigms which is currently the topic of intense research for many fields including medical image segmentation. Like other machine learning approaches to image segmentation, deep learning algorithms use neural networks to classify and segment images. Deep learning methods make use of large, deeply layered and highly connected neural networks to accomplish a certain task; particularly excelling at image analysis. Where they differ is that they can also extract features from image data. Important features are automatically detected by the network and used for segmentation and/or classification. This allows a single deep learning neural network to be used for a variety of image modalities while achieving good results. Because of this, deep learning methods for image segmentation show great promise and are currently being widely researched. It also makes it much easier for non-data scientists to use a deep learning model than other methods, making them a much more accessible tool for researchers and medical professionals alike. They are not without drawbacks, however.

​	Deep learning methods rely on very complex, highly connected, deep neural networks and can be very resource intensive to train on average computers. This is particularly impactful to time-sensitive tasks or when no access to a suitable computer is available. However, this complexity does allow for a greater degree of optimization. For all machine learning algorithms, how well the model is trained is directly proportional to the size of the data set with which it is being trained. These training sets must also be annotated by a medical professional, creating a problem for medical image segmentation because annotated medial image training sets are scarcely available online even though relevant data is being produced daily. This can be mitigated to some extent using data augmentation, but larger data sets would greatly improve performance. Nevertheless, deep learning methods for medical image segmentation are an area intense research and offer an interesting take on computer vision and visual processing because these methods do not involve directly programming the logic into the program to segment images but instead rely on novel approaches to allow the program to *learn* how to extract, identify and classify features from medical images in a manner which allows the program to effectively segment these images with high accuracy and even produce 3D reconstructions of organs and even tissues within organs that have been affected by some disease.

## Artificial Neurons

​	Artificial neural networks are constructs which are designed to mimic the human brain by simulating neurons. Biological neurons function by receiving some set of inputs through dendritic synapses and output some signal through the axon to outgoing synapses. Neurons do not send signals as they are received however; instead, some activation threshold must be reached before an output, called an action potential, is triggered. Effectively, all incoming signals (which can be inhibitory or excitatory) are summated by the neuron and if some threshold is reached, the neuron's action potential is triggered. Artificial neurons perform much the same task.

![](C:\Users\ssbri\OneDrive\UWinCompSci\2020_2_Summer\COMP8510-VisualProcessing\Project\Research Project\a.png)

​	Artificial neurons take a number of various inputs from other neurons or data-structures, perform some summation and activation function and then produce a signal which is sent onwards. These neurons also have other properties which all the designer to modify the signal propagation and backpropagation of the network as a whole. The activation function, for example, can be based off a any mathematical function and basically acts to normalize output to a desired range, typically between 0 and 1 or -1 and 1. The most commonly used activation function is the rectified linear unit (ReLU) function which outputs either 0 or some positive number. ReLU is often used because it is easy to implement and is a non-linear function which exhibits linear behaviour which is very important to avoid vanishing gradients. Neurons use a form of minimalization call gradient descent. Similar to linear regression, finding the line of best fit, gradient descent seeks to reduce the difference in a set of coefficient from a real world values. Gradient descent uses a cost function to measure this difference, by minimizing the cost, the coefficients are brought closer to their expected values. Gradient decent is so-named because it descends along the gradient of the cost function until a minima is reached (ideally the global minimum). The reason this is useful for AI is because gradient descent is capable of minimizing the cost of non-linear functions, which allows for a much more accurate representation how real world variables change due to a vast variety of issues by mapping a set of input variables to a number of outputs. The neuron is able to do this by adjusting the weights of the incoming connections in manner which minimizes cost.

![](C:\Users\ssbri\OneDrive\UWinCompSci\2020_2_Summer\COMP8510-VisualProcessing\Project\Research Project\Picture1.png)

​	It is perhaps now easy to see how an interesting mathematical application can quite accurately (in many cases) enable a neuron to *learn* how to model real-world relationship with just gradient descent alone. What is especially interesting is what occurs if they are combined to form large networks.

## Artificial Neural Networks

​	Artificial neural networks (ANNs) are connections of large layers of neurons with the intent of modelling multifaceted, higher-level relationships. ANNs are also biologically inspired constructs composed of various neurons, similar to the brain. ANN is a broad term which can apply to many different combinations of tens to hundreds of neurons with vastly different degrees of connectivity. At their core, ANNs contain an input layer of neurons which take direct input from data and pass it to an output layer of neurons which perform some sort of operation. Before reaching the output layer, signals may pass through various *hidden layers* which can have a wide array of architectures and connectivity. These hidden layers take input from the the preceding layers and are able to address much more high-level relationships (that is relationships between the outputs of the input layers). In the scope of medical image segmentation, ANNs are typically used for image classification and object detection and the most commonly used ANN for these tasks are convolutional neural networks.

## Deep Learning: Convolutional Neural Networks in Image Segmentation

​	Convolutional neural networks (CNNs), are deep learning artificial neural networks which excel at image classification problems. This is because their architecture allows CNNs to extract important features from images and subsequently learn which of these features are important for a proper classification. As their name suggests, CNN do this by utilizing the principle of convolution. A convolution operation is when a filter is used to map activations within a preceding layer of neurons. By creating entire convolutional layers, a CNN is able to extract and learn valuable information. ![](C:\Users\ssbri\OneDrive\UWinCompSci\2020_2_Summer\COMP8510-VisualProcessing\Project\Research Project\Picture2.png)

​	In the CNN, the flow of data is much the same as with an ANN, what changes is the operations being performed between layers. Importantly, CNNs use convolution and max-pooling layers in addition to just activation layers. From an input/data layer, a convolutional layer measure the input or activation of an area of the preceding layer with a filter, also known as a kernel, and generates a feature map. Next, the activation layer takes the input from this feature map and generates and activation map as an output. The neurons in the activation layer act just like those previous discussed in ANNs. Importantly, a subsampling/pooling layer looks at kernels within the activation layer and produces an output. This layer in particular is used to reduce dimensionality of the previous layer by used a max or average pooling algorithm. This is allows the CNN to learn about the extracted features in the next layer of the CNN which may look exactly like another iteration of this one except with  reduced dimensionality. These properties are what allow CNNs to excel of image analysis because they are able to extract important feature from data in addition to learning about these features. By passing the output to another convolutional layer, activation layer and so-on, a larger CNN is able to learn about increasingly high level features in data. 

![](C:\Users\ssbri\OneDrive\UWinCompSci\2020_2_Summer\COMP8510-VisualProcessing\Project\Research Project\Picture3.png)

​	Taking for example the image of a car, the first convolution, activation and pooling layers in the CNN may be able to learn about low-level features in the image such as lines, curves, while the next layers may learn that combinations of the primitive objects form shapes like circles, rectangle, etc. Proceeding layers may learn how to related where these features are with respect to where other features located in the image. Finally, the output of this part of the CNN might be fed to a classification system which can in-turn learn how to identify objects based on the features present in the image. These are relatively straightforward examples of deep learning neural networks, but more exciting, and retrospectively intuitive architectures such as U-Nets are also widely used.