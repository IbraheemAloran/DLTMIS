{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net for Medical Image Segmentation\n",
    "**Welcome to the U-Net tutorial for Image Segmentation!**\n",
    "\n",
    "For this tutorial we will be using the U-Net architecture from the original U-Net paper by Olaf Ronneberger *et al*.\n",
    "\n",
    "![U-Net from Olaf *et all*](img/Unet.png)\n",
    "\n",
    "## Intro\n",
    "U-Nets are a more recent development in deep learning and are very useful implementations of CNNs. They acquired there name because that have a series of convolutions (called the analysis path) and then deconvolutional steps (called the synthesis path) which reduces and expands the dimensionality of the layers in the network. As a result, they are often depicted as having a U-shape.\n",
    "\n",
    "The analysis path serves to extract features from the input data and acts roughly like previously discussed neural networks. The input layer is passed to a convolutional layer, activation layer, pooling layer and then the next convolutional layer until dimensionality is reduced several times. There are two key differences however. (1) Before each pooling step occurs, an activation map is sent along a shortcut to a corresponding synthesis layer. (2) In the synthetic path, up-convolution is used to map high level features to an activation map from the same level in the analysis path. This activation map is concatenated to up-convolved data and is used to produce the next level of convolution, before up-convolution occurs again. This provides high resolutions features to the deconvolution layers which allows the U-Net to recover the important features in the input image while *filtering* the redundant ones. This is what make U-Net especially great in that image segmentation and causes it to excel at image segmentation because it is able to identify key structures without needing to even understand what these structures are. What's more is that these U-Nets can be combined into even larger networks and/or have other architecture added to them such as dense blocks which can improve their efficacy in many situations.\n",
    "\n",
    "#### U-Net Paper\n",
    "<div class=\"references\">\n",
    "<p>Olaf Ronneberger and Philipp Fischer and Thomas Brox. 2015. <em>U-Net: Convolutional Networks for Biomedical Image Segmentation</em>. <a href=\"https://arxiv.org/abs/1505.04597\">arXiv: 1505.04597</a></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started: Medical Image Segmentation with U-Net\n",
    "**Welcome to the UNet tutorial for Chest X-Ray Image Segmentation!**\n",
    "Medical image segmentation and medical image classification are very different tasks. Classification simply aims to classify images into a number of predefined groups, or classes. For example, the Medical Image Classification tutorial was made to classify chest x-ray images into those which displayed normal pathology or signs of pneumonia.<br/>\n",
    "<br/>\n",
    "Medical image segmentation, on the other hand aims to look at images and segment them into components, using a mask. Specifically, this tutorial will cover semantic segmentation in which we will try to train a fully convolutional neural network to create image masks from chest x-rays that cover the area of the image which shows the lungs; segmenting the image into lung tissue and non-lung tissue.<br/>\n",
    "<br/>\n",
    "The neural network being used here is based off work by *Roenneberger et al* in their paper, <a href=\"https://arxiv.org/abs/1505.04597\">*U-Net: Convolutional Networks for Biomedical Image Segmentation*</a>. We should find that the network will produce much improved results compared to the other CNN tutorial.<br/>\n",
    "<br/>\n",
    "If you've not tried the CNN for Medical Image *Classification* tutorial, complete that one first as in contains a more introductory approach to CNNs. Furthermore, this tutorial uses data from Kaggle to segment lung tissue in chest x-rays. You can get the dataset <a href=\"https://www.kaggle.com/nikhilpandey360/chest-xray-masks-and-labels\">HERE</a>.\n",
    "\n",
    "**You will need to create a verified Kaggle account, download the data and change the source directory in this file.**\n",
    "\n",
    "Let's get started! For brevity, since it was covered in the previous tutorial, will complete all the prep work below without explanation and jump right to the model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "\n",
    "img_dims = 256 # should be atleast 256px\n",
    "my_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "inPath = \"C:\\Datasets\\LungSegmentation\\Lung Segmentation\"\n",
    "\n",
    "train_imgs = os.listdir(inPath + \"/CXR_png\")\n",
    "mask_imgs = os.listdir(inPath + \"/masks\")\n",
    "test_imgs = os.listdir(inPath + \"/test\")\n",
    "\n",
    "X_train = np.zeros((len(train_imgs), img_dims, img_dims, 3), dtype=np.uint8)\n",
    "Y_train = np.zeros((len(mask_imgs), img_dims, img_dims, 1), dtype=np.uint8)\n",
    "\n",
    "for n, file in tqdm(enumerate(train_imgs), total=len(train_imgs)):\n",
    "    t_img = imread(inPath + \"/CXR_png/\" + file)\n",
    "    t_img = resize(t_img, (img_dims, img_dims, 3), mode='constant', preserve_range=True)\n",
    "    X_train[n] = t_img\n",
    "    if Path(inPath + \"/masks/\" + file).exists():\n",
    "        m_mask = imread(inPath + \"/masks/\" + file)\n",
    "    else:\n",
    "        m_mask = imread(inPath + \"/masks/\" + file[:-4] + \"_mask.png\")\n",
    "    m_mask = resize(m_mask, (img_dims, img_dims, 1), mode='constant', preserve_range=True)\n",
    "    Y_train[n] = m_mask\n",
    "    \n",
    "X_test = np.zeros((len(train_imgs), img_dims, img_dims, 3), dtype=np.uint8)\n",
    "\n",
    "for n, file in tqdm(enumerate(test_imgs), total=len(test_imgs)):\n",
    "    test_img = imread(inPath + \"/test/\" + file)\n",
    "    test_img = resize(test_img, (img_dims, img_dims, 3), mode='constant', preserve_range=True)\n",
    "    X_test[n] = test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "Using the UNet diagram as a guide, we can define the model architecture using keras layers as with previous tutorials. The most important feature we need to concider here is the concatination which occurs in the ascending (decoder) path, as mentioned in the intro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by defining the input layer\n",
    "inputs = tf.keras.layers.Input((img_dims, img_dims, 3))\n",
    "\n",
    "#encoder level one (contraction)\n",
    "e1 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
    "e1 = tf.keras.layers.Dropout(rate=0.2)(e1)\n",
    "e1 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same')(e1)\n",
    "\n",
    "e2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(e1)\n",
    "\n",
    "#encoder level 2\n",
    "e2 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(e2)\n",
    "e2 = tf.keras.layers.Dropout(rate=0.2)(e2)\n",
    "e2 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(e2)\n",
    "\n",
    "e3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(e2)\n",
    "\n",
    "#encoder level 3\n",
    "e3 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(e3)\n",
    "e3 = tf.keras.layers.Dropout(rate=0.2)(e3)\n",
    "e3 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(e3)\n",
    "\n",
    "e4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(e3)\n",
    "\n",
    "#encoder level 4\n",
    "e4 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(e4)\n",
    "e4 = tf.keras.layers.Dropout(rate=0.2)(e4)\n",
    "e4 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(e4)\n",
    "\n",
    "e5 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(e4)\n",
    "\n",
    "#encoder level 5\n",
    "e5 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same')(e5)\n",
    "e5 = tf.keras.layers.Dropout(rate=0.2)(e5)\n",
    "e5 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same')(e5)\n",
    "\n",
    "#decoder level 4\n",
    "d4 = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=(2,2), strides=(2,2), padding='same')(e5)\n",
    "d4 = tf.keras.layers.concatenate([d4, e4])\n",
    "d4 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(d4)\n",
    "d4 = tf.keras.layers.Dropout(rate=0.2)(d4)\n",
    "d4 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(d4)\n",
    "\n",
    "#decoder level 3\n",
    "d3 = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=(2,2), strides=(2,2), padding='same')(d4)\n",
    "d3 = tf.keras.layers.concatenate([d3, e3])\n",
    "d3 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(d3)\n",
    "d3 = tf.keras.layers.Dropout(rate=0.2)(d3)\n",
    "d3 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(d3)\n",
    "\n",
    "#decoder level 2\n",
    "d2 = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=(2,2), strides=(2,2), padding='same')(d3)\n",
    "d2 = tf.keras.layers.concatenate([d2, e2])\n",
    "d2 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(d2)\n",
    "d2 = tf.keras.layers.Dropout(rate=0.2)(d2)\n",
    "d2 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(d2)\n",
    "\n",
    "#decoder level 1\n",
    "d1 = tf.keras.layers.Conv2DTranspose(filters=16, kernel_size=(2,2), strides=(2,2), padding='same')(d2)\n",
    "d1 = tf.keras.layers.concatenate([d1, e1])\n",
    "d1 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same')(d1)\n",
    "d1 = tf.keras.layers.Dropout(rate=0.2)(d1)\n",
    "d1 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same')(d1)\n",
    "\n",
    "#output layer\n",
    "outputs = tf.keras.layers.Conv2D(filters=1, kernel_size=(1,1), activation='sigmoid')(d1)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(filepath='xray_model.hdf5', save_best_only=True, save_weights_only=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=3, mode='min')\n",
    "board = TensorBoard(log_dir='logs')\n",
    "\n",
    "my_callbacks = [\n",
    "    checkpoint,\n",
    "    #lr_reduce,\n",
    "    early_stop,\n",
    "    board\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "With the model defined, we can now train it with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            validation_split=0.1,\n",
    "            epochs=my_epochs,\n",
    "            callbacks=my_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training Data\n",
    "We can plot training data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['accuracy', 'loss']):\n",
    "    ax[i].plot(hist.history[met])\n",
    "    ax[i].plot(hist.history['val_' + met])\n",
    "    ax[i].set_title('Model {}'.format(met))\n",
    "    ax[i].set_xlabel('epochs')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'CHNCXR_0025_0.png'\n",
    "img = image.load_img(img_path, target_size=(img_dims, img_dims))\n",
    "pred = image.img_to_array(img)\n",
    "pred = np.expand_dims(pred, axis=0)\n",
    "pred = pred.astype('float32')/255\n",
    "\n",
    "prediction = model.predict(pred)\n",
    "\n",
    "plt.imshow(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "Finally, we should save the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model.save('path/to/file/filename')\n",
    "model.save('UNet_Segmentation_Model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
