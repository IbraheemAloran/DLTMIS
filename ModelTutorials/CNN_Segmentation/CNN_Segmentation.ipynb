{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Medical Image Segmentation\n",
    "\n",
    "**Welcome to the CNN tutorial for Chest X-Ray Image Segmentation!**\n",
    "Medical image segmentation and medical image classification are very different tasks. Classification simply aims to classify images into a number of predefined groups, or classes. For example, the Medical Image Classification tutorial was made to classify chest x-ray images into those which displayed normal pathology or signs of pneumonia.<br/>\n",
    "<br/>\n",
    "Medical image segmentation, on the other hand aims to look at images and segment them into components, using a mask. Specifically, this tutorial will cover semantic segmentation in which we will try to train a fully convolutional neural network to create image masks from chest x-rays that cover the area of the image which shows the lungs; segmenting the image into lung tissue and non-lung tissue.<br/>\n",
    "<br/>\n",
    "The neural network being used here is based off work by *Long et al* in their paper, <a href=\"https://arxiv.org/abs/1411.4038\">*Fully Convolutional Networks for Semantic Segmentation*</a>. We should find that the network will produce somewhat blurry and not very accurate masks. This is somewhat a result of the dataset being used but also of the properties of this neural network's architecture. A more accurate and cutting edge model will be investigated in the UNet and k-Dense UNet tutorials, but it is recommended that you try this one out first!<br/>\n",
    "<br/>\n",
    "If you've not tried the CNN for Medical Image *Classification* tutorial, complete that one first as in contains a more introductory approach to CNNs. Furthermore, this tutorial uses data from Kaggle to segment lung tissue in chest x-rays. You can get the dataset <a href=\"https://www.kaggle.com/nikhilpandey360/chest-xray-masks-and-labels\">HERE</a>.\n",
    "\n",
    "**You will need to create a verified Kaggle account, download the data and change the source directory in this file.**\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages and Dependencies\n",
    "To get started, we need to import the required packages which the code will use. The architecture of the neural network will use Keras. \n",
    "\n",
    "Keras is a high level machine learning API which uses TensorFlow as a backend. This allows users to focus on implementation from a more blackbox perspective. TensorFlow is much better suited for more whitebox approaches. TensorFlow uses a structure called tensors which a basically n-dimensional arrays or matricies. We can use NumPy arrays to pass to TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "\n",
    "img_dims = 256 # should be atleast 256px\n",
    "my_epochs = 20\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Images\n",
    "We now need to set an input path from which to load images. **You must change the following line to point to the correct folder on your system.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inPath = \"C:\\Datasets\\LungSegmentation\\Lung Segmentation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Filepath\n",
    "You can verify that the path has been set correctly by counting the images from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The for loop below simply counts and displays the number of images in each directory. This gives us an idea of how many images we are dealing with in each set. You should confirm that your output is as follows:<br />\n",
    "`Set: /CXR_png\n",
    "Images: 800\n",
    "Set: /masks\n",
    "Images: 704\n",
    "Set: /test\n",
    "Images: 96`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = len(os.listdir(inPath + \"/CXR_png\"))\n",
    "masks = len(os.listdir(inPath + \"/masks\"))\n",
    "tests = len(os.listdir(inPath + \"/test\"))\n",
    "\n",
    "print(\"Set: \" + \"/CXR_png\" + \"\\nImages: \" + str(train) + \"\\nSet: \" + \"/masks\" + \"\\nImages: \" + str(masks) + \"\\nSet: \" + \"/test\" + \"\\nImages: \" + str(tests))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "Confirming that the we have our dataset accessed appropriately, we also need to assess how to properly prepare this data for the model. Upon inspection, it seems like 704 masks are generated from 704 of the original images in CXR_png. By examining the image names, we also find that no masks are created for the 96 test cases. This means we'll need to either (a) organize the images into folders which are more easily accessed or (b) process the data at runtime without reorganizing files. <br/>\n",
    "<br/>\n",
    "Also, upon further inspection, the dimensions of the images being used range from 1130x948 px to nearly 5000x5000 px. This is an important consideration for the determining what the dimensions of input layer of the CNN should be. We should also consider that the masks range in size from 1255x989 px to 4892x4020 px. This needs to be considered in order to determine what size the output layer should be.<br/>\n",
    "<br/>\n",
    "Finally, because of the last point, we make the assumption that resizeing that images will not affect the accuracy of the training (heads up: it will).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the test images repeated in the /CXR_png and /test directories is redundant, so to make things easier, I'm going to clean up /CXR_png by removing all the images that are present in the /test folder. This leaves only training images in the /CXR_png directory which can then be used to gather training images without having to check each image for having a corresponding mask.<br/>\n",
    "<br/>\n",
    "I'll do that with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in os.scandir(inPath + \"/test\"):\n",
    "    if entry.is_file():\n",
    "        if Path(inPath + \"/CXR_png/\" + entry.name):\n",
    "            # print(entry.name)\n",
    "            os.remove(inPath + \"/CXR_png/\" + entry.name)\n",
    "\n",
    "            \n",
    "# Confirm that the command worked as expected\n",
    "train = len(os.listdir(inPath + \"/CXR_png\"))\n",
    "masks = len(os.listdir(inPath + \"/masks\"))\n",
    "tests = len(os.listdir(inPath + \"/test\"))\n",
    "\n",
    "print(\"Set: \" + \"/CXR_png\" + \"\\nImages: \" + str(train) + \"\\nSet: \" + \"/masks\" + \"\\nImages: \" + str(masks) + \"\\nSet: \" + \"/test\" + \"\\nImages: \" + str(tests))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the directories cleaned up, we can now prepare the data. We'll do this a little differently that the CNN for classification tutorial. This time we will create two matrices of zeros which was have their contents filled with the images. One of these matrices will contain all of the training images and the other will contain all of the masks. We read in the data from the files to the new matrices.<br/>\n",
    "<br/>\n",
    "Note that we import a package called tqdm which just provides a progress bar to display while the loop is iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_imgs = os.listdir(inPath + \"/CXR_png\")\n",
    "mask_imgs = os.listdir(inPath + \"/masks\")\n",
    "test_imgs = os.listdir(inPath + \"/test\")\n",
    "\n",
    "X_train = np.zeros((len(train_imgs), img_dims, img_dims, 3), dtype=np.uint8)\n",
    "Y_train = np.zeros((len(mask_imgs), img_dims, img_dims, 1), dtype=np.uint8)\n",
    "\n",
    "for n, file in tqdm(enumerate(train_imgs), total=len(train_imgs)):\n",
    "    t_img = imread(inPath + \"/CXR_png/\" + file)\n",
    "    t_img = resize(t_img, (img_dims, img_dims, 3), mode='constant', preserve_range=True)\n",
    "    X_train[n] = t_img\n",
    "    if Path(inPath + \"/masks/\" + file).exists():\n",
    "        m_mask = imread(inPath + \"/masks/\" + file)\n",
    "    else:\n",
    "        m_mask = imread(inPath + \"/masks/\" + file[:-4] + \"_mask.png\")\n",
    "    m_mask = resize(m_mask, (img_dims, img_dims, 1), mode='constant', preserve_range=True)\n",
    "    Y_train[n] = m_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to prepare the test data. This can be done in a similar manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros((len(train_imgs), img_dims, img_dims, 3), dtype=np.uint8)\n",
    "\n",
    "for n, file in tqdm(enumerate(test_imgs), total=len(test_imgs)):\n",
    "    test_img = imread(inPath + \"/test/\" + file)\n",
    "    test_img = resize(test_img, (img_dims, img_dims, 3), mode='constant', preserve_range=True)\n",
    "    X_test[n] = test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the CNN Architecture\n",
    "The most important step! Keras makes creating neural networks extremely intuitive and easy. In fact, it may seem like all the previous steps are the most difficult to understand initially. <br />\n",
    "To generate this CNN, we must first contruct each layer in the neural network. We'll walk through Abhinav's CNN layer by layer.<br/>\n",
    "But, before we get started, we need to create an input tensor (a matrix) that is the dimensions of the images being used to train the neural network and an added third dimension of 3. This third dimension is due to the three channels used to compose JPEG images (height x width x channel) which are actually three dimensional array data structures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input((img_dims, img_dims, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the first layers in the CNN, which takes `inputs`, the JPEG images, as an input. The convolutional layer scans a kernel across the image and produces another tensor which maps the activation of the kernel across the image. This is the first step in feature extraction. The resulting tensor created from this convolutional layer is then passed to another convolutional layer, and the tensor output by this layer is passed to a maxpooling layer that reduces dimensionality of the data and helps the network to learn and extract features from the image produced the convolution. This completes the first convolutional block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first convolutional block\n",
    "c1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
    "c1 = tf.keras.layers.Dropout(rate=0.2)(c1)\n",
    "c1 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(c1)\n",
    "c1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c1)\n",
    "\n",
    "#second convolutional block\n",
    "c2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(c1)\n",
    "c2 = tf.keras.layers.Dropout(rate=0.2)(c2)\n",
    "c2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(c2)\n",
    "c2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c2)\n",
    "\n",
    "#third convolutional block\n",
    "c3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(c2)\n",
    "c2 = tf.keras.layers.Dropout(rate=0.2)(c3)\n",
    "c3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(c3)\n",
    "c3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c3)\n",
    "\n",
    "#Upsampling and fourth convolutional block\n",
    "u1 = tf.keras.layers.UpSampling2D(size=(2,2))(c3) \n",
    "u1 = tf.keras.layers.concatenate([u1, c2], axis=-1)\n",
    "c4 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(u1)\n",
    "c4 = tf.keras.layers.Dropout(rate=0.2)(c4)\n",
    "c4 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(c4)\n",
    "\n",
    "#Upsampling and fifth convolutional block\n",
    "u2 = tf.keras.layers.UpSampling2D(size=(2,2))(c4) \n",
    "u2 = tf.keras.layers.concatenate([u2, c1], axis=-1)\n",
    "c5 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(u2)\n",
    "c5 = tf.keras.layers.Dropout(rate=0.2)(c5)\n",
    "c5 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(c5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer\n",
    "outputs = tf.keras.layers.Conv2D(filters=1, kernel_size(1,1), activation='sigmoid')(c5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling The Model\n",
    "Now, we can compile the model. But before we do that, there are some things to consider.<br/>\n",
    "1. Training can take a very long time, from hours possibly to days <br/>\n",
    "2. If training is interupted, then progress is lost <br/>\n",
    "3. We can't know the what the perfect number of epochs is that is high enough to prevent underfitting, but low enough to prevent overfitting \n",
    "\n",
    "\n",
    "In order to prevent losing a model during training, we can implement the `ModelCheckpoint` callback. This call back saves the model after every epoch to a file at specified `filepath`. If the `save_best_only` boolean is set to `True` then the model will save only the best epochs. The `save_weights_only` boolean means that instead of saving all parameters in the model, just the weights of each connection are saved. <br/>\n",
    "<br/>\n",
    "Choosing hyperparameters like epoch, can be done using trial and error, but this is tedious and may cause overfitting if the model is allowed to train for too many epochs. In order to prevent overfitting, we can used both the `ReduceLROnPlateau` and `EarlyStopping` callbacks. The the `ReduceLROnPLateau` callback method reduces the learning rate of the model once learning starts to plateau. The `EarlyStopping` callback stops the training process when a certain metric that is being monitored stops improving.<br/><br/>\n",
    "For `ReduceLROnPlateau`, the value which we'd like to monitor is the model's loss, so `monitor` is set to `val_loss`. The factor value is factor by which the learning rate will be reduced. `patience` is the number of consecutive epochs with no improvements to wait before reducing learning rate, while `verbose` simply provides a warning message saying that the learning rate is being reduced. Finally, `mode` tells the method to look for improvements in increasing loss when set to `'max'`. When set to `'min'`, the method looks for decreasing loss. In total, this method is saying, monitor`val_loss` as it approaches some `max`imum. If the loss fails to increase for `2` consecutive epochs, reduce the learning rate by a factor of `0.3` and warn the user.<br/><br/>\n",
    "For `EarlyStopping`, the value being monitor is again `val_loss`, just as in the previous method. There is also another parameter, `min_delta` which is set to `0.1`. This parameter specifies the minimum change in the monitored value to be concidered an improvement. `mode` is set to `min` this time because we want the method to detect when `val_loss` reduces. Finally, the `patience` parameter is set to 3 so as to not interfere with `ReduceLSOnPlateau`.\n",
    "<br/>\n",
    "<br/>\n",
    "To read more about the <a href=\"https://keras.io/api/callbacks/reduce_lr_on_plateau/\">`ReduceLROnPlateau`</a> and <a href=\"https://keras.io/api/callbacks/early_stopping/\">`EarlyStopping`</a> methods (actually they are classes), then see the appropriate documentation by clicking them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model and compiling\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(filepath='xray_model.hdf5', save_best_only=True, save_weights_only=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=3, mode='min')\n",
    "board = TensorBoard(log_dir='logs')\n",
    "\n",
    "my_callbacks = [\n",
    "    checkpoint,\n",
    "    #lr_reduce,\n",
    "    early_stop,\n",
    "    board\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Finally, we can train the model! \n",
    "<br/>\n",
    "The `model.fit()` method trains the model and stores training history to help visualize and track the training process. \n",
    "<br/>\n",
    "We're using a few parameters here with `model.fit()`. The first, `train_gen`, passes the training set generator which was definined above with the line: <br/>\n",
    "`train_gen, test_gen, test_data, test_labels = process_data(img_dims, batch_size)`.<br/> \n",
    "Next, the `epochs` parameter sets the number of training runs, epochs, to perform training on. We defined `my_epochs` above to be 100. The `callbacks` parameter is used to set the callbacks we want the program to use during training. These were diffined above as `my_callbacks`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            validation_split=0.1,\n",
    "            epochs=my_epochs,\n",
    "            callbacks=my_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the history, we can create loss and accuracy plots to visualize model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['accuracy', 'loss']):\n",
    "    ax[i].plot(hist.history[met])\n",
    "    ax[i].plot(hist.history['val_' + met])\n",
    "    ax[i].set_title('Model {}'.format(met))\n",
    "    ax[i].set_xlabel('epochs')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also gather more useful data like accuracy and pricision by calculating a confusion matrix and other test metrics. Abhinav uses the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Models\n",
    "So you've trained the model, great. But we want to actually use the model that we've trained, otherwise, what good is it?\n",
    "<br/>\n",
    "Luckily, TensorFlow and Keras make it very easy to save a complete model for further use and/or further training later.\n",
    "<br/>\n",
    "This can be done very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model.save('path/to/file/filename')\n",
    "model.save('CNN_Segementation_example.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Prediction\n",
    "Now, we'll try a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normal X-ray should score closely to 0\n",
    "img_path = 'CHNCXR_0025_0.png'\n",
    "img = image.load_img(img_path, target_size=(144, 144))\n",
    "pred = image.img_to_array(img)\n",
    "pred = np.expand_dims(pred, axis=0)\n",
    "pred = pred.astype('float32')/255\n",
    "\n",
    "prediction = model.predict(pred)\n",
    "\n",
    "plt.imshow(prediction[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONE!\n",
    "Now that you've completed the tutorial, you should go through and experiment with different hyperparameters. <br/><br/>\n",
    "I would recommend considering the following:\n",
    "1. Should both `ReduceLROnPlateau` and `EarlyStopping` be used?\n",
    "2. If both are used, what parameters should be changed to ensure they don't interfere with eachother and still allow the model to train? (hit, you should have at least 10-15 epochs for this particular model with this data to be trained effectively.\n",
    "3. How does adjusting batch size affect learning and the speed of learning?\n",
    "4. What structural changes can be made to the CNN that could make it perform better? (I would definitely exeperiment here with different CNN architectures!)\n",
    "5. After you've experimented with this data, try to modify to the code to use another dataset of your choice. Importantly, if you do so, you must ensure that the images are atleast 150x150 px in size.\n",
    "6. Try changing the CNN to take a larger image size. Does is take longer to train? Is it more accurate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
